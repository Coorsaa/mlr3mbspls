% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TunerSeqMBsPCA.R
\name{TunerSeqMBsPCA}
\alias{TunerSeqMBsPCA}
\title{Sequential Component-wise Tuner for Group-Sparse MB-sPCA}
\value{
A tuned \code{TuningInstance} (invisibly). The result is written to the instance
via \code{assign_result()} with \verb{learner_param_vals = list(c_matrix = <matrix>)}
and the best inner objective value under the key \code{"mbspca.mean_ev"}.
}
\description{
\code{TunerSeqMBsPCA} tunes one component at a time for \code{PipeOpMBsPCA}, choosing
a block-sparsity vector \strong{c} that maximises \strong{cross-validated total variance
explained}. After each component it refits on the full residuals, performs
an optional permutation test for significance, and—if significant—deflates
all blocks before proceeding to the next component.

The public interface mirrors \code{TunerSeqMBsPLS}; only the objective (variance
explained rather than latent correlation) and the C++ back-end differ.
}
\section{Works with}{

A learner whose pipeline contains a \code{PipeOpMBsPCA} named \code{"mbspca"}. The
tuner writes the chosen √(L¹) budgets into \code{param_vals$c_matrix}
(\verb{blocks × components}) of that pipeop.
}

\section{Construction}{

\verb{TunerSeqMBsPCA$new(tuner = "random_search", budget = 100L,}
\verb{resampling = mlr3::rsmp("cv", folds = 3), parallel = "none",}
\verb{early_stopping = TRUE, n_perm = 1000L, perm_alpha = 0.05)}
}

\section{Optimisation procedure}{

For component \emph{k}, a search space over integer proxies \verb{c_b ∈ [1, p_b]} is
defined per block (\emph{p_b} = #features). The objective evaluates CV mean of
total variance explained. The optimal integers are mapped to √(L¹) budgets
via \code{sqrt(c_b)} and stored in column \emph{k} of \code{c_matrix}. Residuals are
deflated and the procedure continues for the next component (up to the
requested maximum, possibly shortened by early stopping).
}

\examples{
\dontrun{
library(mlr3)
library(mlr3pipelines)
library(mlr3tuning)
blocks = list(eng = c("disp", "hp", "drat"), body = c("wt", "qsec"))
po = PipeOpMBsPCA$new(blocks = blocks, param_vals = list(ncomp = 3))
lrn = as_learner(po \%>>\% po("regr.ranger"))

ti = TuningInstanceSingleCrit$new(
  task = tsk("mtcars"),
  learner = lrn,
  resampling = rsmp("holdout"),
  measure = msr("regr.rmse"),
  search_space = ps(), # outer space unused; tuner overrides mbspca internals
  terminator = trm("none")
)
TunerSeqMBsPCA$new(budget = 50)$optimize(ti)
ti$result_learner_param_vals$c_matrix
}

}
\seealso{
\link{PipeOpMBsPCA}, \link[mlr3tuning:Tuner]{mlr3tuning::Tuner}, \link[bbotk:bbotk-package]{bbotk::bbotk}
}
\concept{mb-sPCA}
\section{Super class}{
\code{\link[mlr3tuning:Tuner]{mlr3tuning::Tuner}} -> \code{TunerSeqMBsPCA}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-TunerSeqMBsPCA-new}{\code{TunerSeqMBsPCA$new()}}
\item \href{#method-TunerSeqMBsPCA-optimize}{\code{TunerSeqMBsPCA$optimize()}}
\item \href{#method-TunerSeqMBsPCA-clone}{\code{TunerSeqMBsPCA$clone()}}
}
}
\if{html}{\out{
<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TunerSeqMBsPCA-new"></a>}}
\if{latex}{\out{\hypertarget{method-TunerSeqMBsPCA-new}{}}}
\subsection{Method \code{new()}}{
Create a new TunerSeqMBsPCA.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSeqMBsPCA$new(
  tuner = "random_search",
  budget = 100L,
  resampling = rsmp("cv", folds = 3),
  parallel = "none",
  early_stopping = TRUE,
  n_perm = 1000L,
  perm_alpha = 0.05
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{tuner}}{character(1). Inner optimizer ID (e.g., "random_search").}

\item{\code{budget}}{integer(1). Number of evaluations per component (default 100).}

\item{\code{resampling}}{Resampling. Template for inner CV (default \code{rsmp("cv", folds = 3)}).}

\item{\code{parallel}}{character(1). "none" or "inner" (future-based parallel fold evals).}

\item{\code{early_stopping}}{logical(1). Enable permutation early stopping (default TRUE).}

\item{\code{n_perm}}{integer(1). Number of permutations if early stopping is enabled.}

\item{\code{perm_alpha}}{numeric(1). Significance level for permutation test.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TunerSeqMBsPCA-optimize"></a>}}
\if{latex}{\out{\hypertarget{method-TunerSeqMBsPCA-optimize}{}}}
\subsection{Method \code{optimize()}}{
Run the sequential tuning loop.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSeqMBsPCA$optimize(instance)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{instance}}{\link[mlr3tuning:TuningInstanceSingleCrit]{mlr3tuning::TuningInstanceSingleCrit} (or compatible). The outer instance to populate.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
The modified instance (invisibly).
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TunerSeqMBsPCA-clone"></a>}}
\if{latex}{\out{\hypertarget{method-TunerSeqMBsPCA-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSeqMBsPCA$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
